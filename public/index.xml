<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>#Probllama by OpenShield</title>
    <link>http://localhost:1313/</link>
    <description>Recent content on #Probllama by OpenShield</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 25 Jul 2024 05:00:25 +0200</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Google Colab AI: Data Leakage Through Image Rendering Fixed. Some Risks Remain.</title>
      <link>http://localhost:1313/posts/2024-07-25/google_colab_ai_data_leakage_through_image_rendering_fixed_some_risks_remain.md/</link>
      <pubDate>Thu, 25 Jul 2024 05:00:25 +0200</pubDate>
      <guid>http://localhost:1313/posts/2024-07-25/google_colab_ai_data_leakage_through_image_rendering_fixed_some_risks_remain.md/</guid>
      <description>Google Colab AI, now just called Gemini in Colab, was vulnerable to data leakage via image rendering. This is an older bug report, dating back to November 29, 2023. However, recent events prompted me to write this up: Google did not reward this finding, and Colab now automatically puts Notebook content (untrusted data) into the prompt. Let‚Äôs explore the specifics. Google Colab AI - Revealing the System Prompt At the end of November last year, I noticed that there was a ‚ÄúColab AI‚Äù feature, which integrated an LLM to chat with and write code.</description>
    </item>
    <item>
      <title>Breaking Instruction Hierarchy in OpenAI\&#39;s gpt-4o-mini</title>
      <link>http://localhost:1313/posts/2024-07-22/breaking_instruction_hierarchy_in_openais_gpt_4o_mini/</link>
      <pubDate>Mon, 22 Jul 2024 06:14:05 -0700</pubDate>
      <guid>http://localhost:1313/posts/2024-07-22/breaking_instruction_hierarchy_in_openais_gpt_4o_mini/</guid>
      <description>Recently, OpenAI announced gpt-4o-mini and there are some interesting updates, including safety improvements regarding ‚ÄúInstruction Hierarchy‚Äù: OpenAI puts this in the light of ‚Äúsafety‚Äù, the word security is not mentioned in the announcement. Additionally, this The Verge article titled ‚ÄúOpenAI‚Äôs latest model will block the ‚Äòignore all previous instructions‚Äô loophole‚Äù created interesting discussions on X, including a first demo bypass. I spent some time this weekend to get a better intuition about gpt-4o-mini model and instruction hierarchy, and the conclusion is that system instructions are still not a security boundary.</description>
    </item>
    <item>
      <title>Sorry, ChatGPT Is Under Maintenance: Persistent Denial of Service through Prompt Injection and Memory Attacks</title>
      <link>http://localhost:1313/posts/2024-07-08/sorry_chatgpt_is_under_maintenance_persistent_denial_of_service_through_prompt_injection_and_memory_attacks/</link>
      <pubDate>Mon, 08 Jul 2024 14:30:18 -0700</pubDate>
      <guid>http://localhost:1313/posts/2024-07-08/sorry_chatgpt_is_under_maintenance_persistent_denial_of_service_through_prompt_injection_and_memory_attacks/</guid>
      <description>Imagine you visit a website with ChatGPT, and suddenly, it stops working entirely! In this post we show how an attacker can use prompt injection to cause a persistent denial of service that lasts across chat sessions for a user. Hacking Memories Previously we discussed how ChatGPT is vulnerable to automatic tool invocation of the memory tool. This can be used by an attacker during prompt injection to ingest malicious or fake memories into your ChatGPT.</description>
    </item>
    <item>
      <title>üö® New CVE Alert: CVE-2024-37032 - Ollama Remote Code Execution üö®</title>
      <link>http://localhost:1313/posts/2024-06-23/cve_2024_37032_ollama_remote_code_execution/</link>
      <pubDate>Sun, 23 Jun 2024 21:00:17 -0800</pubDate>
      <guid>http://localhost:1313/posts/2024-06-23/cve_2024_37032_ollama_remote_code_execution/</guid>
      <description>Wiz podcast CVE-2024-37032</description>
    </item>
    <item>
      <title>GitHub Copilot Chat: From Prompt Injection to Data Exfiltration</title>
      <link>http://localhost:1313/posts/2024-06-14/github_copilot_chat_from_prompt_injection_to_data_exfiltration/</link>
      <pubDate>Fri, 14 Jun 2024 21:00:17 -0800</pubDate>
      <guid>http://localhost:1313/posts/2024-06-14/github_copilot_chat_from_prompt_injection_to_data_exfiltration/</guid>
      <description>This post highlights how the GitHub Copilot Chat VS Code Extension was vulnerable to data exfiltration via prompt injection when analyzing untrusted source code. GitHub Copilot Chat GitHub Copilot Chat is a VS Code Extension that allows a user to chat with source code, refactor code, get info about terminal output, or general help about VS Code, and things along those lines. It does so by sending source code, along with the user‚Äôs questions to a large language model (LLM).</description>
    </item>
    <item>
      <title>DPD error caused chatbot to swear at customer</title>
      <link>http://localhost:1313/posts/2024-01-19/dpd_error_caused_chatbot_to_swear_at_customer/</link>
      <pubDate>Fri, 19 Jan 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/2024-01-19/dpd_error_caused_chatbot_to_swear_at_customer/</guid>
      <description>DPD has disabled part of its online support chatbot after it swore at a customer. The parcel delivery firm uses artificial intelligence (AI) in its online chat to answer queries, in addition to human operators. A new update caused it to behave unexpectedly, including swearing and criticizing the company.&#xA;DPD said it had disabled the part of the chatbot that was responsible and was updating its system as a result. &amp;ldquo;An error occurred after a system update yesterday.</description>
    </item>
    <item>
      <title>Archive</title>
      <link>http://localhost:1313/archives/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/archives/</guid>
      <description>archives</description>
    </item>
  </channel>
</rss>

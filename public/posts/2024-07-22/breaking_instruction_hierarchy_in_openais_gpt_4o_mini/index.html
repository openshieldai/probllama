<!doctype html><html lang=en-us><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><style type=text/css>body{font-family:monospace}</style><title>Breaking Instruction Hierarchy in OpenAI\'s gpt-4o-mini</title>
<link rel=stylesheet href=/css/style.css></head><body><header>==============================<br>== <a href=http://localhost:1313/>#Probllama by OpenShield</a> ==<br>==============================<div style=float:right></div><br><p><nav><a href=/><b>Home</b></a>.
<a href=/index.xml><b>RSS</b></a>.</nav></p></header><main><article><h1>Breaking Instruction Hierarchy in OpenAI\'s gpt-4o-mini</h1><b><time>2024-07-22 06:14:05</time></b><div><p>Recently, OpenAI announced gpt-4o-mini and there are some interesting updates, including safety improvements regarding “Instruction Hierarchy”: OpenAI puts this in the light of “safety”, the word security is not mentioned in the announcement. Additionally, this The Verge article titled “OpenAI’s latest model will block the ‘ignore all previous instructions’ loophole” created interesting discussions on X, including a first demo bypass. I spent some time this weekend to get a better intuition about gpt-4o-mini model and instruction hierarchy, and the conclusion is that system instructions are still not a security boundary.
<a href=https://embracethered.com/blog/posts/2024/chatgpt-gpt-4o-mini-instruction-hierarchie-bypasses/>More details here</a></p></div></article></main><aside><div><div><h3>LATEST POSTS</h3></div><div><ul><li><a href=/posts/2024-07-25/google_colab_ai_data_leakage_through_image_rendering_fixed_some_risks_remain.md/>Google Colab AI: Data Leakage Through Image Rendering Fixed. Some Risks Remain.</a></li><li><a href=/posts/2024-07-22/breaking_instruction_hierarchy_in_openais_gpt_4o_mini/>Breaking Instruction Hierarchy in OpenAI\'s gpt-4o-mini</a></li><li><a href=/posts/2024-07-08/sorry_chatgpt_is_under_maintenance_persistent_denial_of_service_through_prompt_injection_and_memory_attacks/>Sorry, ChatGPT Is Under Maintenance: Persistent Denial of Service through Prompt Injection and Memory Attacks</a></li></ul></div></div></aside><footer><p>&copy; 2024 <a href=http://localhost:1313/><b>#Probllama by OpenShield</b></a>.
<a href=https://github.com/openshieldai/probllama><b>Github</b></a>.
<a href=https://openshield.ai><b>OpenShield</b></a>.</p></footer></body></html>